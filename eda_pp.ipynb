{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basis\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Paellete\n",
    "palette = [\"#2D2926FF\", \"#E94B3CFF\"]\n",
    "color_palette = sns.color_palette(palette)\n",
    "\n",
    "# Remove Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set the option to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/group_datagroups_new/0.1/grouped_data.csv\")\n",
    "df = pd.read_csv(\"data/groups/0.1/grouped_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count nan\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null 'group_id' values with 8\n",
    "df[\"group_id\"] = df[\"group_id\"].fillna(8)\n",
    "\n",
    "# Count nan again\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"pol\", \"psa\", \"n_donors\", \"nrotb\", \"n_acceptors\", \"logP\"]\n",
    "X = df[features]\n",
    "y = df[\"dG_exp\"]\n",
    "groups = df[\"group_id\"]\n",
    "id_column = \"mobleyID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[features + [\"dG_exp\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_distributions(df, features, figsize=(8, 6), output_dir=None, custom_bins=None):\n",
    "    \"\"\"\n",
    "    Create publication-quality KDE plots for each feature with detailed statistics and consistent bar alignment.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the features\n",
    "    features (list): List of feature names to plot\n",
    "    figsize (tuple): Figure size (width, height) for each plot\n",
    "    output_dir (str, optional): Directory to save plots. If None, plots are displayed.\n",
    "    custom_bins (dict, optional): Dictionary specifying the number of bins for specific features.\n",
    "\n",
    "    Returns:\n",
    "    list: List of generated figure objects\n",
    "    \"\"\"\n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"dark\")  # Use a darker color palette for publication quality\n",
    "\n",
    "    # List to store figures\n",
    "    figures = []\n",
    "\n",
    "    # Plot each feature\n",
    "    for col in features:\n",
    "        # Create a new figure for each feature\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Determine the number of bins\n",
    "        if custom_bins and col in custom_bins:\n",
    "            bins = custom_bins[col]\n",
    "        else:\n",
    "            bins = \"auto\"\n",
    "        # bins = None\n",
    "\n",
    "        # Create histogram with consistent bar alignment\n",
    "        sns.histplot(\n",
    "            data=df,\n",
    "            x=col,\n",
    "            stat=\"density\",\n",
    "            kde=True,\n",
    "            ax=ax,\n",
    "            color=\"#1f77b4\",  # Darker blue\n",
    "            alpha=0.6,\n",
    "            bins=bins,  # Use custom bins\n",
    "            line_kws={\"linewidth\": 2.5},  # Thicker KDE line\n",
    "        )\n",
    "\n",
    "        # Calculate statistics\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        median = df[col].median()\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "\n",
    "        # Create statistics text\n",
    "        stats_text = (\n",
    "            f\"Mean: {mean:.2f}\\n\"\n",
    "            f\"Std: {std:.2f}\\n\"\n",
    "            f\"Median: {median:.2f}\\n\"\n",
    "            f\"Q1: {q1:.2f}\\n\"\n",
    "            f\"Q3: {q3:.2f}\"\n",
    "        )\n",
    "\n",
    "        # Add statistics text\n",
    "        ax.text(\n",
    "            0.95,\n",
    "            0.95,\n",
    "            stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment=\"top\",\n",
    "            horizontalalignment=\"right\",\n",
    "            fontsize=10,  # Increase font size for text box\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.9),\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_title(f\"Distribution of {col}\", fontsize=16, pad=10)\n",
    "        ax.set_xlabel(col, fontsize=14)\n",
    "        ax.set_ylabel(\"Density\", fontsize=14)\n",
    "\n",
    "        # Increase axis line width\n",
    "        ax.spines[\"top\"].set_linewidth(1.5)\n",
    "        ax.spines[\"right\"].set_linewidth(1.5)\n",
    "        ax.spines[\"left\"].set_linewidth(1.5)\n",
    "        ax.spines[\"bottom\"].set_linewidth(1.5)\n",
    "\n",
    "        # Increase tick size\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "\n",
    "        # Add rug plot for better visualization\n",
    "        sns.rugplot(data=df, x=col, ax=ax, color=\"gray\", alpha=0.5)\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save or display the plot\n",
    "        if output_dir:\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            plt.savefig(\n",
    "                os.path.join(output_dir, f\"{col}_distribution.png\"), dpi=300\n",
    "            )  # Publication quality\n",
    "            plt.close(fig)  # Close the figure to free up memory\n",
    "        else:\n",
    "            figures.append(fig)\n",
    "\n",
    "    # If not saving, return list of figures\n",
    "    return figures if output_dir is None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify custom bins for 'psa' and 'pol'\n",
    "custom_bins = {\n",
    "    \"psa\": 10,\n",
    "    \"pol\": 8,\n",
    "    \"n_donors\": 7,\n",
    "    \"n_acceptors\": 1,\n",
    "    \"nrotb\": 1,\n",
    "}\n",
    "\n",
    "# Call the function with custom bins\n",
    "figures = plot_distributions(\n",
    "    df,\n",
    "    features=features,\n",
    "    custom_bins=custom_bins,\n",
    "    output_dir=\"plots/eda/\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df[features + [\"dG_exp\"]].corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = df[\"group_id\"].value_counts()\n",
    "print(\"Group distribution:\")\n",
    "print(group_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "group_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Distribution of Groups\")\n",
    "plt.xlabel(\"Group ID\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axs[i].scatter(df[feature], df[\"dG_exp\"])\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel(\"dG_exp\")\n",
    "    axs[i].set_title(f\"{feature} vs dG_exp\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby(\"group_id\")[features + [\"dG_exp\"]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variables = [\"dG_exp\", \"pol\", \"psa\", \"n_donors\"]\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    data = df[var]\n",
    "\n",
    "    # Calculate IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "    # Plot\n",
    "    axs[i].boxplot(data)\n",
    "    axs[i].scatter(np.ones(len(outliers)), outliers, color=\"red\", s=20)\n",
    "    axs[i].set_title(var)\n",
    "\n",
    "    print(f\"{var}:\")\n",
    "    print(f\"Number of outliers: {len(outliers)}\")\n",
    "    print(f\"Percentage of outliers: {len(outliers) / len(data) * 100:.2f}%\")\n",
    "    print(f\"Range of outliers: {outliers.min()} to {outliers.max()}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# List of variables to analyze\n",
    "variables = [\"pol\", \"psa\", \"n_donors\", \"nrotb\", \"n_acceptors\", \"dG_exp\"]\n",
    "\n",
    "# Calculate skewness\n",
    "skewness = df[variables].apply(lambda x: skew(x))\n",
    "\n",
    "print(\"Skewness for each variable:\")\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "\n",
    "\n",
    "class OutlierRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column=\"dG_exp\"):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        data = df[self.column]\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(data >= lower_bound) & (data <= upper_bound)]\n",
    "        return df\n",
    "\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pol_transformer=None):\n",
    "        self.pol_transformer = pol_transformer\n",
    "        self.columns = [\n",
    "            \"pol\",\n",
    "            \"n_acceptors\",\n",
    "            \"n_donors\",\n",
    "            \"nrotb\",\n",
    "            \"psa\",\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.pol_transformer.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[self.columns] = self.pol_transformer.transform(X_[self.columns])\n",
    "        return X_\n",
    "\n",
    "\n",
    "class CustomStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        columns=[\n",
    "            \"pol\",\n",
    "            \"psa\",\n",
    "            \"logP\",\n",
    "            \"n_acceptors\",\n",
    "            \"n_donors\",\n",
    "            \"nrotb\",\n",
    "        ],\n",
    "    ):\n",
    "        self.columns = columns\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[self.columns] = self.scaler.transform(X_[self.columns])\n",
    "        return X_\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"outlier_remover\", OutlierRemover()),\n",
    "        (\n",
    "            \"custom_transformer\",\n",
    "            CustomTransformer(pol_transformer=PowerTransformer(method=\"yeo-johnson\")),\n",
    "        ),\n",
    "        (\"standard_scaler\", CustomStandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Apply the pipeline\n",
    "df_processed = preprocessing_pipeline.fit_transform(df)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Processed shape:\", df_processed.shape)\n",
    "\n",
    "# Check the first few rows of the processed data\n",
    "print(df_processed.head())\n",
    "\n",
    "# Verify the transformations\n",
    "print(\"\\nMean of scaled features:\")\n",
    "print(df_processed[[\"pol\", \"psa\", \"dG_exp\", \"logP\"]].mean())\n",
    "print(\"\\nStandard deviation of scaled features:\")\n",
    "print(df_processed[[\"pol\", \"psa\", \"dG_exp\", \"logP\"]].std())\n",
    "\n",
    "print(\"\\nSkewness of log-transformed 'pol':\", skew(df_processed[\"pol\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(15, 12))\n",
    "\n",
    "# Flatten the 2D array of axes, but keep it as a 2D array\n",
    "axs_flat = axs.flatten()\n",
    "\n",
    "for i, col in enumerate(features + [\"dG_exp\"]):\n",
    "    if i < 6:\n",
    "        # For the first two rows, use axes as normal\n",
    "        ax = axs_flat[i]\n",
    "    else:\n",
    "        # For the last plot, use the center axis in the last row\n",
    "        ax = axs[2, 1]\n",
    "\n",
    "    ax.hist(df_processed[col], bins=30)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "# Remove the unused subplots\n",
    "fig.delaxes(axs[2, 0])\n",
    "fig.delaxes(axs[2, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# List of variables to analyze\n",
    "variables = [\"pol\", \"psa\", \"n_donors\", \"nrotb\", \"n_acceptors\", \"dG_exp\"]\n",
    "\n",
    "# Calculate skewness\n",
    "skewness = df_processed[variables].apply(lambda x: skew(x))\n",
    "\n",
    "print(\"Skewness for each variable:\")\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as a csv file\n",
    "df_processed.to_csv(\"groups_new/0.1/grouped_data_without_outliers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"custom_transformer\",\n",
    "            CustomTransformer(pol_transformer=PowerTransformer(method=\"yeo-johnson\")),\n",
    "        ),\n",
    "        (\"standard_scaler\", CustomStandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Apply the pipeline\n",
    "df_processed_with_outliers = preprocessing_pipeline.fit_transform(df)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Processed shape:\", df_processed.shape)\n",
    "\n",
    "# Check the first few rows of the processed data\n",
    "print(df_processed_with_outliers.head())\n",
    "\n",
    "# Verify the transformations\n",
    "print(\"\\nMean of scaled features:\")\n",
    "print(df_processed_with_outliers[[\"pol\", \"psa\", \"dG_exp\", \"logP\"]].mean())\n",
    "print(\"\\nStandard deviation of scaled features:\")\n",
    "print(df_processed_with_outliers[[\"pol\", \"psa\", \"dG_exp\", \"logP\"]].std())\n",
    "\n",
    "print(\"\\nSkewness of log-transformed 'pol':\", skew(df_processed_with_outliers[\"pol\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe as a csv file\n",
    "df_processed_with_outliers.to_csv(\n",
    "    \"groups_new/0.1/grouped_data_with_outliers.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
