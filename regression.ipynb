{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basis\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    GroupKFold,\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "# Perform Randomized Search for Tree-Based Models\n",
    "\n",
    "# Paellete\n",
    "palette = [\"#2D2926FF\", \"#E94B3CFF\"]\n",
    "color_palette = sns.color_palette(palette)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/groups/0.1/grouped_data_without_outliers.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"pol\", \"psa\", \"n_donors\", \"nrotb\", \"n_acceptors\", \"logP\"]\n",
    "X = df[features]\n",
    "y = df[\"dG_exp\"]\n",
    "groups = df[\"group_id\"]\n",
    "id_column = \"mobleyID\"\n",
    "n_splits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Groups shape: {groups.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"RÂ² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store indices of training and testing sets\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# Desired proportion\n",
    "train_size = 0.8  # 80% training data\n",
    "test_size = 0.2  # 20% testing data\n",
    "\n",
    "# Get unique group_ids\n",
    "group_ids = df[\"group_id\"].unique()\n",
    "\n",
    "# Loop through each group\n",
    "for group in group_ids:\n",
    "    group_data = df[df[\"group_id\"] == group]\n",
    "    group_indices = group_data.index.tolist()\n",
    "\n",
    "    # Perform train_test_split on group data\n",
    "    group_train_indices, group_test_indices = train_test_split(\n",
    "        group_indices,\n",
    "        train_size=train_size,\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    # Append indices to the respective lists\n",
    "    train_indices.extend(group_train_indices)\n",
    "    test_indices.extend(group_test_indices)\n",
    "\n",
    "# Create training and testing sets\n",
    "train_df = df.loc[train_indices]\n",
    "test_df = df.loc[test_indices]\n",
    "\n",
    "# Separate features and target variables\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[\"dG_exp\"]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[\"dG_exp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GroupStratifiedKFold:\n",
    "    \"\"\"\n",
    "    Custom cross-validator that provides train/test indices to split data proportionally within groups.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, shuffle=True, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        # Ensure groups are provided\n",
    "        if groups is None:\n",
    "            raise ValueError(\"The 'groups' parameter should not be None\")\n",
    "\n",
    "        # Initialize fold indices\n",
    "        fold_indices = [[] for _ in range(self.n_splits)]\n",
    "\n",
    "        # Get unique groups\n",
    "        unique_groups = np.unique(groups)\n",
    "\n",
    "        # Set random state\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        # Process each group\n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            group_indices = np.where(groups == group)[0]\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(group_indices)\n",
    "\n",
    "            # Split group indices into folds\n",
    "            n_samples = len(group_indices)\n",
    "            fold_sizes = np.full(self.n_splits, n_samples // self.n_splits, dtype=int)\n",
    "            fold_sizes[: n_samples % self.n_splits] += 1  # Distribute the remainder\n",
    "            current = 0\n",
    "            for i in range(self.n_splits):\n",
    "                start, stop = current, current + fold_sizes[i]\n",
    "                fold_indices[i].extend(group_indices[start:stop])\n",
    "                current = stop\n",
    "\n",
    "        # Generate train/test indices for each fold\n",
    "        for i in range(self.n_splits):\n",
    "            test_indices = np.array(fold_indices[i])\n",
    "            train_indices = np.array(\n",
    "                [\n",
    "                    idx\n",
    "                    for fold in fold_indices\n",
    "                    if fold is not fold_indices[i]\n",
    "                    for idx in fold\n",
    "                ]\n",
    "            )\n",
    "            yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom splitter\n",
    "n_splits = 4\n",
    "cv = GroupStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross-validator\n",
    "group_kfold = GroupKFold(n_splits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# Define models and their hyperparameters\n",
    "linear_models = {\"Ridge\": Ridge(), \"Lasso\": Lasso()}\n",
    "\n",
    "linear_params = {\n",
    "    \"Ridge\": {\"alpha\": [0.1, 1.0, 10.0, 100.0]},\n",
    "    \"Lasso\": {\"alpha\": [0.001, 0.01, 0.1, 1.0, 10.0]},\n",
    "}\n",
    "\n",
    "# Perform Grid Search for Linear Models\n",
    "for name in linear_models:\n",
    "    print(f\"Training {name} model...\")\n",
    "    grid = GridSearchCV(\n",
    "        estimator=linear_models[name],\n",
    "        param_grid=linear_params[name],\n",
    "        cv=group_kfold.split(X, y, groups),\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid.fit(X, y)\n",
    "    print(f\"Best parameters for {name}: {grid.best_params_}\")\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(f\"Performance of {name}:\")\n",
    "    evaluate_model(y, y_pred)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Define models and their hyperparameters\n",
    "tree_models = {\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(random_state=42, objective=\"reg:squarederror\"),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "tree_params = {\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"max_depth\": [None, 10, 20],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2],\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.05, 0.1],\n",
    "        \"max_depth\": [3, 5],\n",
    "        \"num_leaves\": [31, 50],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "    },\n",
    "}\n",
    "\n",
    "tree_best_params = {}\n",
    "\n",
    "\n",
    "for name in tree_models:\n",
    "    print(f\"Training {name} model...\")\n",
    "    if name in [\"XGBoost\", \"LightGBM\"]:\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=tree_models[name],\n",
    "            param_distributions=tree_params[name],\n",
    "            n_iter=20,\n",
    "            cv=group_kfold.split(X, y, groups),\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(\n",
    "            estimator=tree_models[name],\n",
    "            param_grid=tree_params[name],\n",
    "            cv=group_kfold.split(X, y, groups),\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "    search.fit(X, y)\n",
    "    print(f\"Best parameters for {name}: {search.best_params_} \\n\")\n",
    "    tree_best_params[name] = search.best_params_\n",
    "    best_model = search.best_estimator_\n",
    "    y_pred = best_model.predict(X)\n",
    "    print(f\"Performance of {name}:\")\n",
    "    evaluate_model(y, y_pred)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score\n",
    "\n",
    "# Assuming df, X, y, groups, and tree_best_params are already defined\n",
    "\n",
    "# Define the models with best parameters\n",
    "best_models = {\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        **tree_best_params[\"RandomForest\"], random_state=42\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(\n",
    "        **tree_best_params[\"GradientBoosting\"], random_state=42\n",
    "    ),\n",
    "    \"XGBoost\": xgb.XGBRegressor(\n",
    "        **tree_best_params[\"XGBoost\"], random_state=42, objective=\"reg:squarederror\"\n",
    "    ),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(**tree_best_params[\"LightGBM\"], random_state=42),\n",
    "}\n",
    "\n",
    "best_model_predictions = {}\n",
    "\n",
    "# Perform cross-validation\n",
    "n_splits = 5\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"Performing cross-validation for {name}...\")\n",
    "\n",
    "    # Use MAE as the scoring metric\n",
    "    mae_scores = cross_val_score(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=group_kfold.split(X, y, groups),\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Convert negative MAE to positive MAE\n",
    "    mae_scores = -mae_scores\n",
    "\n",
    "    print(f\"Cross-validation results for {name}:\")\n",
    "    print(f\"Mean MAE: {np.mean(mae_scores):.4f} (+/- {np.std(mae_scores) * 2:.4f})\")\n",
    "    print(f\"Individual fold MAEs: {mae_scores}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Now you have trained models in the final_models dictionary\n",
    "# You can use these for further analysis or predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    return rmse, mae, r2, pearson_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models and make predictions\n",
    "final_models = {}\n",
    "predictions = {}\n",
    "metrics = {}\n",
    "feature_importances = {}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"Training final {name} model on entire dataset...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    final_models[name] = model\n",
    "\n",
    "    # Make predictions on train and test sets\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # Store predictions\n",
    "    predictions[name] = {\"train\": train_preds, \"test\": test_preds}\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    train_metrics = calculate_metrics(y_train, train_preds)\n",
    "    test_metrics = calculate_metrics(y_test, test_preds)\n",
    "\n",
    "    metrics[name] = {\"train\": train_metrics, \"test\": test_metrics}\n",
    "\n",
    "    # Store feature importances\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        feature_importances[name] = model.feature_importances_\n",
    "    elif hasattr(model, \"coef_\"):\n",
    "        feature_importances[name] = model.coef_\n",
    "    else:\n",
    "        print(f\"Warning: Feature importance not available for {name}\")\n",
    "\n",
    "    print(f\"{name} model trained.\")\n",
    "    print(\n",
    "        f\"Train metrics - RMSE: {train_metrics[0]:.4f}, MAE: {train_metrics[1]:.4f}, R2: {train_metrics[2]:.4f}, Pearson: {train_metrics[3]:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Test metrics - RMSE: {test_metrics[0]:.4f}, MAE: {test_metrics[1]:.4f}, R2: {test_metrics[2]:.4f}, Pearson: {test_metrics[3]:.4f}\"\n",
    "    )\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Optional: Print feature importances\n",
    "for name, importances in feature_importances.items():\n",
    "    print(f\"\\nFeature importances for {name}:\")\n",
    "    for feature, importance in zip(X_train.columns, importances):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors and symbols\n",
    "colors = {\"train\": \"#4477AA\", \"test\": \"#FF0000\"}\n",
    "symbols = {\"train\": \"circle\", \"test\": \"triangle-up\"}\n",
    "\n",
    "# Iterate through models and create separate plots\n",
    "for i, (name, model) in enumerate(final_models.items(), 1):\n",
    "    # Create a new figure for each model\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for training data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_train,\n",
    "            y=predictions[name][\"train\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Training\",\n",
    "            marker=dict(\n",
    "                size=8, color=colors[\"train\"], symbol=symbols[\"train\"], opacity=0.6\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add traces for test data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_test,\n",
    "            y=predictions[name][\"test\"],\n",
    "            mode=\"markers\",\n",
    "            name=\"Test\",\n",
    "            marker=dict(\n",
    "                size=8, color=colors[\"test\"], symbol=symbols[\"test\"], opacity=0.6\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add diagonal line (y=x) for reference\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min(y_train.min(), y_test.min()), max(y_train.max(), y_test.max())],\n",
    "            y=[min(y_train.min(), y_test.min()), max(y_train.max(), y_test.max())],\n",
    "            mode=\"lines\",\n",
    "            name=\"y=x\",\n",
    "            showlegend=False,\n",
    "            line=dict(color=\"#000000\", dash=\"dot\", width=1.5),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add plot title\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=name,\n",
    "            font=dict(size=24, family=\"Arial Black\", color=\"black\"),\n",
    "            x=0.5,\n",
    "            y=0.95,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add evaluation metrics\n",
    "    test_metrics = metrics[name][\"test\"]\n",
    "    metrics_text = (\n",
    "        f\"<b>Test Set Metrics</b>:<br>\"\n",
    "        f\"RMSE: {test_metrics[0]:.4f}<br>\"\n",
    "        f\"RÂ²: {test_metrics[2]:.4f}<br>\"\n",
    "        f\"Pearson's r: {test_metrics[3]:.4f}<br>\"\n",
    "        f\"MAE: {test_metrics[1]:.4f}\"\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=0.98,\n",
    "        y=0.02,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        xanchor=\"right\",\n",
    "        yanchor=\"bottom\",\n",
    "        text=metrics_text,\n",
    "        showarrow=False,\n",
    "        font=dict(size=14, family=\"Arial Black\"),\n",
    "        align=\"left\",\n",
    "        bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        width=800,\n",
    "        legend=dict(\n",
    "            font=dict(size=12, family=\"Arial Black\"),\n",
    "            x=0.05,  # Changed from 0.5 to 0.05\n",
    "            y=0.95,  # Changed from -0.15 to 0.95\n",
    "            xanchor=\"left\",  # Changed from \"center\" to \"left\"\n",
    "            yanchor=\"top\",\n",
    "            orientation=\"v\",  # Changed from \"h\" to \"v\"\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.7)\",\n",
    "            bordercolor=\"Black\",\n",
    "            borderwidth=1,\n",
    "        ),\n",
    "        plot_bgcolor=\"white\",\n",
    "        font=dict(family=\"Arial Black\"),\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"ÎG<sub>exp</sub> (kcal/mol)\",\n",
    "        title_font=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "        tickfont=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "        tickformat=\".1f\",\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=2,\n",
    "        mirror=True,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"ÎG<sub>pred</sub> (kcal/mol)\",\n",
    "        title_font=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "        tickfont=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "        tickformat=\".1f\",\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=2,\n",
    "        mirror=True,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # fig.write_html(f\"plots/{name.replace(' ', '_').lower()}_plot.html\")\n",
    "    fig.write_image(f\"plots/residuals/{name.replace(' ', '_').lower()}_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "color_scheme = [\n",
    "    \"#6a0dad\",  # Dark purple\n",
    "    \"#4682b4\",  # Steel blue\n",
    "    \"#556b2f\",  # Dark olive green\n",
    "    \"#8b4513\",  # Saddle brown\n",
    "    \"#2e8b57\",  # Sea green\n",
    "    \"#d2691e\",  # Chocolate\n",
    "    \"#708090\",  # Slate gray\n",
    "    \"#cd5c5c\",  # Indian red\n",
    "]\n",
    "\n",
    "\n",
    "# Ensure enough colors are available for all features by repeating the color scheme\n",
    "def get_color_scheme(num_features):\n",
    "    return color_scheme * (num_features // len(color_scheme) + 1)\n",
    "\n",
    "\n",
    "# Iterate through models and create feature importance plots\n",
    "for name, importances in feature_importances.items():\n",
    "    # Sort features by importance\n",
    "    sorted_idx = importances.argsort()\n",
    "    sorted_features = X_train.columns[sorted_idx]\n",
    "    sorted_importances = importances[sorted_idx]\n",
    "\n",
    "    # Get a color scheme for this specific plot\n",
    "    colors = get_color_scheme(len(sorted_features))\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add bar trace for feature importances\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=sorted_features,  # Changed to x to make bars vertical\n",
    "            y=sorted_importances,\n",
    "            marker_color=colors[\n",
    "                : len(sorted_features)\n",
    "            ],  # Use different colors for each feature\n",
    "            name=\"Feature Importance\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"Feature Importances for {name}\",\n",
    "            font=dict(size=24, family=\"Arial Black\", color=\"black\"),\n",
    "            x=0.5,\n",
    "            y=0.95,\n",
    "        ),\n",
    "        height=600,  # Fixed height for vertical bars\n",
    "        width=max(\n",
    "            800, len(sorted_features) * 40\n",
    "        ),  # Adjust width based on number of features\n",
    "        plot_bgcolor=\"white\",\n",
    "        font=dict(family=\"Arial Black\"),\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"Features\",\n",
    "        title_font=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "        tickfont=dict(size=12, family=\"Arial Black\", color=\"black\"),\n",
    "        showgrid=False,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=2,\n",
    "        mirror=True,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"Importance\",\n",
    "        title_font=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "        tickfont=dict(size=12, family=\"Arial Black\", color=\"black\"),\n",
    "        showgrid=True,\n",
    "        gridcolor=\"lightgray\",\n",
    "        zeroline=False,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=2,\n",
    "        mirror=True,\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "    # Optionally, save the figure\n",
    "    fig.write_html(f\"plots/{name.replace(' ', '_').lower()}_feature_importance.html\")\n",
    "    fig.write_image(f\"plots/{name.replace(' ', '_').lower()}_feature_importance.png\")\n",
    "\n",
    "# Create a combined plot for all models\n",
    "num_models = len(feature_importances)\n",
    "fig = make_subplots(\n",
    "    rows=num_models, cols=1, subplot_titles=list(feature_importances.keys())\n",
    ")\n",
    "\n",
    "for i, (name, importances) in enumerate(feature_importances.items(), start=1):\n",
    "    sorted_idx = importances.argsort()\n",
    "    sorted_features = X_train.columns[sorted_idx]\n",
    "    sorted_importances = importances[sorted_idx]\n",
    "\n",
    "    # Get a color scheme for this specific subplot\n",
    "    colors = get_color_scheme(len(sorted_features))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=sorted_features,  # Vertical bars in the combined plot\n",
    "            y=sorted_importances,\n",
    "            marker_color=colors[\n",
    "                : len(sorted_features)\n",
    "            ],  # Different colors for each feature\n",
    "            name=name,\n",
    "            showlegend=False,\n",
    "        ),\n",
    "        row=i,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"Feature Importances Across Models\",\n",
    "        font=dict(size=24, family=\"Arial Black\", color=\"black\"),\n",
    "        x=0.5,\n",
    "        y=0.95,\n",
    "    ),\n",
    "    height=300 * num_models,\n",
    "    width=1000,\n",
    "    plot_bgcolor=\"white\",\n",
    "    font=dict(family=\"Arial Black\"),\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Features\",\n",
    "    title_font=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "    tickfont=dict(size=12, family=\"Arial Black\", color=\"black\"),\n",
    "    showgrid=False,\n",
    "    linecolor=\"black\",\n",
    "    linewidth=2,\n",
    "    mirror=True,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Importance\",\n",
    "    title_font=dict(size=14, family=\"Arial Black\", color=\"black\"),\n",
    "    tickfont=dict(size=12, family=\"Arial Black\", color=\"black\"),\n",
    "    showgrid=True,\n",
    "    gridcolor=\"lightgray\",\n",
    "    zeroline=False,\n",
    "    linecolor=\"black\",\n",
    "    linewidth=2,\n",
    "    mirror=True,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"plots/feature_importance/combined_feature_importance.html\")\n",
    "fig.write_image(\"plots/feature_importance/combined_feature_importance.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Define group names\n",
    "group_names = [\n",
    "    \"Alkanol\",\n",
    "    \"Alkanone\",\n",
    "    \"Alkene\",\n",
    "    \"Alkyl Alkanoate\",\n",
    "    \"Halo Alkanes\",\n",
    "    \"Aromatic\",\n",
    "    \"Non-aromatic cyclic\",\n",
    "    \"Nitrogen based\",\n",
    "    \"Misc\",\n",
    "]\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    pearson, _ = pearsonr(y_true, y_pred)\n",
    "    return rmse, mae, r2, pearson\n",
    "\n",
    "\n",
    "# Get the Random Forest model\n",
    "rf_model = final_models[\"RandomForest\"]\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through each group\n",
    "for group in range(9):\n",
    "    # Filter data for the current group\n",
    "    mask = groups == group\n",
    "    X_group = X[mask]\n",
    "    y_group = y[mask]\n",
    "    ids_group = df.loc[mask, id_column]\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_group)\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse, mae, r2, pearson = calculate_metrics(y_group, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    results.append(\n",
    "        {\n",
    "            \"Group\": group_names[group],\n",
    "            \"Count\": len(y_group),\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R2\": r2,\n",
    "            \"Pearson\": pearson,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Store individual predictions\n",
    "    for true, pred, id in zip(y_group, y_pred, ids_group):\n",
    "        results.append(\n",
    "            {\n",
    "                \"Group\": group_names[group],\n",
    "                \"MobleyID\": id,\n",
    "                \"True_dG\": true,\n",
    "                \"Predicted_dG\": pred,\n",
    "                \"Error\": pred - true,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Separate summary and individual results\n",
    "summary_df = results_df[results_df[\"MobleyID\"].isna()].drop(\n",
    "    columns=[\"MobleyID\", \"True_dG\", \"Predicted_dG\", \"Error\"]\n",
    ")\n",
    "individual_df = results_df[results_df[\"MobleyID\"].notna()]\n",
    "\n",
    "# Display summary results\n",
    "print(\"Groupwise Performance Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_true = y\n",
    "overall_pred = rf_model.predict(X)\n",
    "overall_rmse, overall_mae, overall_r2, overall_pearson = calculate_metrics(\n",
    "    overall_true, overall_pred\n",
    ")\n",
    "\n",
    "print(\"\\nOverall Performance:\")\n",
    "print(f\"RMSE: {overall_rmse:.4f}\")\n",
    "print(f\"MAE: {overall_mae:.4f}\")\n",
    "print(f\"R2: {overall_r2:.4f}\")\n",
    "print(f\"Pearson: {overall_pearson:.4f}\")\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "summary_df.to_csv(\"rf_groupwise_summary.csv\", index=False)\n",
    "individual_df.to_csv(\"rf_groupwise_individual.csv\", index=False)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=(\"RMSE\", \"MAE\", \"RÂ²\", \"Pearson Correlation\"),\n",
    "    vertical_spacing=0.2,  # Increased vertical spacing\n",
    "    horizontal_spacing=0.1,\n",
    ")\n",
    "\n",
    "# Add traces for each metric\n",
    "metrics = [\"RMSE\", \"MAE\", \"R2\", \"Pearson\"]\n",
    "positions = [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "]\n",
    "\n",
    "for metric, pos in zip(metrics, positions):\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=summary_df[\"Group\"],\n",
    "            y=summary_df[metric],\n",
    "            name=metric,\n",
    "            marker_color=colors,\n",
    "            text=summary_df[metric].round(3),\n",
    "            textposition=\"outside\",\n",
    "            textfont=dict(size=10),\n",
    "        ),\n",
    "        row=pos[0],\n",
    "        col=pos[1],\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    width=1200,\n",
    "    title_text=\"Groupwise Performance Metrics for Random Forest Model\",\n",
    "    showlegend=False,\n",
    "    font=dict(family=\"Arial\", size=12),\n",
    ")\n",
    "\n",
    "# Update all xaxes\n",
    "fig.update_xaxes(\n",
    "    tickangle=45,\n",
    "    tickfont=dict(size=10),\n",
    "    title_text=\"\",\n",
    ")\n",
    "\n",
    "# Update all yaxes\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Value\",\n",
    "    tickfont=dict(size=10),\n",
    "    title_font=dict(size=12),\n",
    ")\n",
    "\n",
    "# Add a color legend\n",
    "legend_trace = go.Scatter(\n",
    "    x=[None],\n",
    "    y=[None],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        color=colors,\n",
    "        size=10,\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    legendgroup=\"colorlegend\",\n",
    "    legendgrouptitle_text=\"Groups\",\n",
    "    name=\"Groups\",\n",
    ")\n",
    "\n",
    "for name, color in zip(group_names, colors):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color=color, size=10),\n",
    "            showlegend=True,\n",
    "            name=name,\n",
    "            legendgroup=\"colorlegend\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Adjust legend layout\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.2,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        font=dict(size=10),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save the plot\n",
    "fig.write_html(\"plots/groupwise/rf_groupwise_performance.html\")\n",
    "fig.write_image(\"plots/groupwise/rf_groupwise_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
